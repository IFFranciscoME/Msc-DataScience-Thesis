
# -- -------------------------------------------------------------------------- QUESTIONS AND REFERENCES -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- types of normalization scores
https://en.wikipedia.org/wiki/Normalization_(statistics)

# -- AUC vs Logloss as cost functions, which is better for binary classification
Whereas the AUC is computed with regards to binary classification with a varying decision threshold,
logloss actually takes "certainty" of classification into account.

How to choose optimum probability threshold from ROC ?, since the ROC is calculated with variations
of the threshold used to classify into one clase or the other an observation. 

Therefore to my understanding, logloss conceptually goes beyond AUC and is especially relevant in cases
with imbalanced data or in case of unequally distributed error cost 
(for example detection of a deadly disease).

https://stats.stackexchange.com/questions/322408/logloss-vs-gini-auc
https://stats.stackexchange.com/questions/235089/optimizing-auc-vs-logloss-in-binary-classification-
problems

# -- Usefull Difference between AUC & ACC
https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy
http://notmatthancock.github.io/2015/11/11/auc-and-accuracy.html

# -- Akaike Criteria
In order to compare between models and between different parameter configurations of the same model

# -- Simpson paradox 
Simpson's paradox is one of many reasons why itâ€™s important to evaluate your models on different slices 
of data. Model 1 outperforms Model 2 on group A and group B separately, but model 2 can still 
outperform model 1 with the overall data.

https://en.wikipedia.org/wiki/Simpson%27s_paradox
Here is a multivariable procedure that explicitly maximizes classification accuracy and prevents
paradoxical confounding. [5, 6]
https://www.researchgate.net/post/How_do_you_explain_higher_discrimination_using_a_SVM_score_
index_unsupervised_from_a_supervised_SVM_model_using_same_dataset

# -- How does batch size affect convergence of SGD and why
https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why

# -- How to Control the Stability of Training Neural Networks With the Batch Size
https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks
-with-gradient-descent-batch-size/

# -- Number of hidden layers / number of neurons & Universal Aproximation Theorem.
https://www.researchgate.net/post/How-to-decide-the-number-of-hidden-layers-and-nodes-in-a-hidden-layer
https://www.heatonresearch.com/2017/06/01/hidden-layers.html
https://en.wikipedia.org/wiki/Universal_approximation_theorem
https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-
in-a-feedforward-neural-netw

# -- Relationship of numbner of samples and input variables to implementing regularization
Perhaps those previous errors are due to the fact of numnber of samples, number of input variables and regularization
https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/

# -- Saturation in the activation function values at the output layer
https://datascience.stackexchange.com/questions/16911/how-does-sigmoid-saturate-with-large-weights

# -- How to determine Threshold for sigmoid function output and its relation with unbalaced class
https://datascience.stackexchange.com/questions/22639/how-to-determine-threshold-in-sigmoid-function
https://www.cs.bham.ac.uk/~jlw/sem2a2/Web/LearningSigmoid.htm

One alternative is weights on the classes (more to the less frequent class), this appears to be restrictive to
the observed classes in the dataset but not necessarly of the underlying process that produces such observations, and
therefore it could be prone to increasing the generalization error. Another alternative is that dynamically update
the probability threshold to classify the predicted outpus, but this requires a 100% certainty in the ground truth
labels, whether are binary discrete classes or a high level of certainty in how a continuous value is classified as 
1 or 0. The proposition for the dynamic threshold is considering using bayes, the proportion of classes is calculated
with the onhand data as an intial expected class balance and update that numbner with new evidence of class balance within each new fold, with the option of including a divergence factor fiven by the KLDivergence of the previous data and the
present data. 

# -- RandomForest
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

# -- ------------------------------------------------------------------------------------ T-FOLD PROCESS -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Fold-Size: Quarter (48-Fold), Semester (24-Fold), Year (12-Fold), Bi-year (6-Fold), 80-20 (2-Fold)

# -- Within each T-Fold

# -- -- DATA SEPARATION
# -- -- (OPTION 1) Separate data in 80% (training) - 20% (validation)
# -- -- (OPTION 2) Use 100% of the Fold as training

# -- -- EMBARGO
# -- -- -- (OPTION 1) Perform Embargo (drop N observations of data at the beging of each train): Arbitrary
# -- -- -- (OPTION 2) Perform Embargo (drop M observations of data at the beging of each train): max(ACF,PACF)
# -- -- -- (OPTION 3) Do not perform Embargo

# -- -- DATA PRE-PROCESSING (Normalization)

# -- -- (OPTION 1) Before Feature Engineering
# -- -- -- (OPTION 1) Scale x/max(x)
# -- -- -- (OPTION 2) Standard (x-mean(x))/sd(x)
# -- -- -- (OPTION 3) Robust (x - median(x))/IQR

# -- -- (OPTION 2) After Feature Engineering
# -- -- -- (OPTION 1) Scale x/max(x)
# -- -- -- (OPTION 2) Standard (x-mean(x))/sd(x)
# -- -- -- (OPTION 3) Robust (x - median(x))/IQR

# -- -- Conduct the hyperparameter optimization for all models (Genetic Algorithms)
# -- -- Select Hyperparameters for the Genetic Algorithms:
# -- -- -- Crossover, Mutation, Generations, Population, Tournament, HallOfFame

# -- -- Select the fitness metric for the optimization
        (OPTION 1) AUC with train data
        (OPTION 2) AUC with val data
        (OPTION 3) diff AUC with val and train data
        (OPTION 4) mean AUC with val and train data
        (OPTION 5) weighted mean AUC with innersplit proportions (e.g. 80-20) 
        (OPTION 6) inverse weighted mean AUC with inverted innersplit proportions (e.g. 20-80) 

        (OPTION 1) ACC with train data
        (OPTION 2) ACC with val data
        (OPTION 3) diff ACC with val and train data
        (OPTION 4) mean ACC with val and train data
        (OPTION 5) weighted mean ACC with innersplit proportions (e.g. 80-20) 
        (OPTION 6) inverse weighted mean ACC with inverted innersplit proportions (e.g. 20-80)

        (OPTION 1) LOGLOSS with train data
        (OPTION 2) LOGLOSS with val data
        (OPTION 3) diff LOGLOSS with val and train data
        (OPTION 4) mean LOGLOSS with val and train data
        (OPTION 5) weighted mean LOGLOSS with innersplit proportions (e.g. 80-20) 
        (OPTION 6) inverse weighted mean LOGLOSS with inverted innersplit proportions (e.g. 20-80)

# -- -- Save the created input parameters for every fold
# -- -- Evaluate and Save the HoF with best N individuals (hyperparameters and execution parameters configurations)
# -- -- Calculate model metrics for the entire Hall of Fame

# -- ---------------------------------------------------------------------- SYMBOLIC FEATURE ENGINEERING -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Autoregressive features
# -- Hadamard features

# -- Genetic Programming for Symbolic Features
# -- -- iterative genetic parameters: Functions, Population, Generations, Tournament, HoF, output features
# -- -- Heuristics 
# -- -- -- Fitness for feature selection: Largest pearson coefficient between
		   symbolic continuous variables & target binary variable
# -- -- -- Feature parsimony

# -- ----------------------------------------------------------------------------------- MODEL DIVERSITY -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Basic - Logistic Regression with regularization
# -- Statistical Learning - Support Vector Machines with L1 regularization
# -- Machine Learning - Aritificial Neural Network - Multi Layer Pereceptron
# -- (Pending) Tree-based-ensemble method - RandomForest or XGBoost

# -- ------------------------------------------------------------------------------------- MODEL TESTING -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- USING EXTENSIVE FOLD-TESTS DATA:
# -- -- Find the parameter set that produces the max, min value of the Choosen Performance Metric
# -- -- Find the number of occurrences of every unique parameter set (extract the modes)

# -- -- Individual Evaluation: Test the model and the choosen parameter set with the rest of forward global data.
# -- -- Individual Prediction: Test the model and the choosen parameter set with unseen TEST global data.

# -- -- Ensemble Prediction: For every Fold size, find the Best in the HoF at every Fold and predict
		with unseen TEST global data (using a voting decision system)

# ------------------------------------------------------------------------------------------- CHANGE LOG -- #
# ------------------------------------------------------------------------------------------- ---------- -- #

# -- 10.02.2021

- create a new document.py file with all the codes necessary to produce the elements that will be
  added to the thesis document.

- T-Fold process completed

- Enhacement: refactor project structure to support parallelization
- Addition: Log registration with every worker in the paralelization

- mapped all variations of factors to run the general experiment 
- - different types of data pre-processing (Normalize, Standarize, Robust Standarization)
- - data pre-processing order (pre/post features)

- - fitness metric as cost function (optimization process)
	ACC: (train, test, simple, weighted, inv-weighted)
	AUC: (simple, weighted, inv-weighted)
	LOGLOSS: (simple, weighted, inv-weighted)

- - fitness metric as ranking function (selection of cases max, min)
	ACC: (train, test, simple, weighted, inv-weighted)
	AUC: (simple, weighted, inv-weighted)
	LOGLOSS: (simple, weighted, inv-weighted)

- Addition: Asyncronous parallelization capabilities for several functions
- Enhacement: Aesthetic Logs and invidiual files for worker-log during parallelization
- Addition: pickle_rick data saving method for individual file during parallelization

# -- 11.02.2021

- Addition: data_profile function for OHLC and timeseries data profiling
- Addition: data profile for target variable (balance of classes)
- Addition: data profile for feature variables (descriptive statistics)
- Addition: table Correlation between features and features vs target variable

# -- 12.02.2021 

- Enhacement: Variations of AUC, LogLoss, ACC to use as cost functions and ranking function
- Enhacement: search for min, max, mode cases according to selected cost function
- Enhacement: ROC plot to support Multiple ROC values of the evaluated HoF
- Addition: table for multi modal cases

- Enhacement: Global evaluation function for the whole HoF 
- Fix: gray candles (windows without prediction) and vertical line traintest ohlc class
- Results: Execute code with several features to have wider results to analize.

# -- 13.02.2021

- Optimization: add to model_metrics the other sofisticated metrics already calculated
- Optimization: refactor and simplify code in genetic_algo_evaluate 
- Enhacement: add inner-split option of iterations (choose inner test split of data)
- Addition: met_cases params and metric data of the min and max cases
- Enhacement: Refactor project in order to work with no train-test split in inner data sets of the t-folds

# -- 14.02.2021
- Fix: model fit and predict separation in model evaluation
- Enhacement: Train&Test option for plots

# -- 17.02.2021
- Enhacement: Install GPU capabilities in T490

# -- 18.02.2021
- Enhacement: Migrate from sklearn to keras (basic tests)
- Modification: Hide tensorflow console messages

# -- 19.02.2021
- Fix: use pickle to save tf.keras model 
- Fix: global evaluation test with pickle object with tf.keras model (use of model.predict())

# -- 22.02.2021
- Enhacement: Expand hyperparameter to search for ann-mlp model 
- Fix: sklearn wrapper for tf.keras in order to use it with current genetic_algo_optimization
- Fix: snippets and tricks to avoid errors when numeric instability is present in loss function (Futile)

# -- 23.02.2021
- Addition: Create new prices files with H8 intervals for grouping the minute files
- Addition: Added historical prices from 2009 to 2020, and 2021 will be left as ultimate test
- Fix: Error with tf evaluating loss function when producing nan, with callback type TerminateOnNaN()
- Addition: add another metric for optimization: Difference between train and validation metric result
- Addition: added KLDivergence metric and callback with monitoring to it.
- Addition: added callback EarlyStopping monitoring accuracy to it.

# -- 24.02.2021
- Fix: pickle tf.keras model with wrapper from scikit learn. 

# -- 25.02.2021
- Modification: Change nomenclature from Train-Test, to Train-Validation and Test (never seen data from 2021)

# -- 26.02.2021
- Addition: Implement embargo criteria for t-fold variation

# -- 27.02.2021
- Modification: Erase hadamard features, they add model complexity without being uniquely useful
- fix: Filtration of explanatory variables in the target variable, fixed to have  [y_t+1 | X_t]
- Testing: Data - t_folds, folds_embargo, data_profile, data_scaler
- Testing: Features Engineering - linear_features, genetic_programed_features
- Testing: Models - logistic-elasticnet, l1-svm, ann-mlp
- Enhacement: Include polynomial kernel and some values for the degree hyperparameter for SVC
- Testing: Optimization - genetic_algo_optimization

# -- 28.02.2021
- fix: Error in data_scaler, it was scaling 1: cols, leaving out the 1st, probably the cause of saturation
       in ann-mlp model since the 1st has a big scale.
- fix: Erros in predicted probabilities type of data and code refactor in tf_metrics

- (goal) the input features for global evaluation and test evaluation has to be pre-processes exactly
			as were the features used in the optimization 
- (goal) steps and instructions to run project on cluster  

# ---------------------------------------------------------------------------------------------- BACKLOG -- # 
# ---------------------------------------------------------------------------------------------- ------- -- #

# -- NECESSARY

- (pending) the input features for global evaluation and test evaluation has to be pre-processes exactly
			as were the features used in the optimization 

- (pending) Testing: Evaluation - model_evaluation, global_evaluation
- (pending) Results - model_cases

- (pending) Add the forecasting benchmark of y_t+1 = y_t

- (pending) write content and code to define and search for data to test 1 clear hypothesis for the project

- (pending) Refactor tf_model_metrics and sk_model_metrics to not use train/test condition (just any data)

- (pending) add RandomForest classification model with scikit learn
- (pending) Refactor code for timeseries data of metric evolution g_timeseries_auc

- (pending) reduce time elapsed by global evaluation function
- (pending) look for repeated or not used data that is beign saved, pickle files are getting heavier

- (pending) function for average or AIC/BIC for parsimony analysis of symbolic features in fold

- (pending) visual profile: boxplot matrix with outliers for feature variables
- (pending) visual profile: histograms for feature variables
- (pending) visual profile: Correlation heatmap for feature variables
- (pending) visual profile: ROC curves of all HoF in the fold

- (pending) translate every comment to english
- (pending) translate every variable name to english

# -- GOOD TO HAVE

- (pending) guide to multi-gpu & distributed training.

***************************************************************************************** UNORGANIZED NOTES 
***********************************************************************************************************

# -- NOTES FOR FINAL PRESENTATION

- Para que sirve todo esto para mejorar el desempeÃ±o de modelos respecto a un procedimiento clasico?

- como justificar el inv-weighted y proporciones

- Para el caso de Ensamble, como randomforest, utilizar todos los casos en lugar de hacer los 3 mejores. 

# -- For each model show
# -- -- Objective function
# -- -- Cost function 
# -- -- Learning/Optimizing algorithm (Genetic algorithms because categorical data)

# -- -- DATA ANALYSIS
# -- -- -- Amount/dates of data
# -- -- -- Balance of the clases

# -- -- MODEL ANALYSIS
# -- -- -- change in accuracy
# -- -- -- change in auc
# -- -- -- change in parameters

# -- PARAMETER ESTABILITY
# -- -- for every model in every T-Fold evaluate the best and worst of the HoF of the fold
# -- -- display timeseries parameters dataframe (parameters of all bests/worsts of all the folds)
# -- -- display timeseries metrics plots (of all bests/worsts of all the folds)
# -- -- -- AUC, ACC
# -- -- display all the ROC curves and average ROC curve plot for every model

# -- Possible questions to answer:
# -- -- is this method useful to find a parameter set that does not change a lot in time ?
# -- -- is this method useful to have a stable generalization error of the model ?
# -- -- does this method shows that a linear combination of non-linear features performs well
        in financial timeseries forecasting ?
# -- -- what role plays the feature complexity in this method ?
# -- -- What is the time complexity of this method ? 
# -- -- What is the memory complexity of this method ? 

# -- ANN-MLP

# cross analysis (Winning Fold Vs Global Test)
# -- outliers vs regularization
# -- symmetry vs regularization
# -- correlation vs regularization
# -- parsimony vs outliers
# -- parsimony vs symmetry

# model response to features from Fold to Global
# -- Regularization effects in logistic regression
# -- kernel in SVM
# -- training time in ANN-MLP

# -- NOTES ON EXPERIMENTS 

# -- the order and nomenclature of sub-sets is: Train (stays the same), Validation (what conpcetually was test), 
Test a sub-data set used at the very final stage (never seen by any model under any situation)

# -- instead of auc-test, logloss-test, acc-test, i will add auc-diff, logloss-diff, acc-diff as the result of the
difference of the metric in train and in validation sets

# -- apparently, L1 and L2 regularizers for the kernel (applied to weights), is in the order of 0.001

# -- the ratio of generations is useful to be sufficient, like 10 or more, in order to provide several oportunities
of weight initialization at each block of epochs. That is, not varying too much the hyper parameters in order to
for the same hyperparameters find through several iterations the optimal weights.But after 8, it stopped to provide
sufficiently different individuals. 

# -- Dropout is applied to each hidden layer (so it is not applied to the input and the output). One connection
could be that getting the dropped inputs and compare them to the statistical properties of the inputs in that
particular fold, whether they were pre-processed (scaled or normalized)

# -- Note on simmetry when using integers as clases, if use -1, 0, +1, they have a simmetry at 0 ant apparently that
causes numerical instability in the output of the loss function (for the case of cross-entropy)

# -- ERRORS
tensorflow.python.framework.errors_impl.InvalidArgumentError:  assertion failed: [predictions must be >= 0] 
[Condition x >= y did not hold element-wise:] [x (ann-mlp/output_layer/Sigmoid:0) = ] [[-nan][-nan][-nan]...] 
[y (Cast_8/x:0) = ] [0]

tensorflow.python.framework.errors_impl.InvalidArgumentError:  assertion failed: [predictions must be >= 0]
 [Condition x >= y did not hold element-wise:] [x (ann-mlp/output_layer/Sigmoid:0) = ] [[0][0][0]...]
  [y (Cast_8/x:0) = ] [0]

- Big momentums appear to provoque nan error, so momentum 0.4 as max appear not to cause the error

- added an arbitrary result for both classes, to guarantee that everytime, at least, will be 1 of each class
  and that way the error of nans is avoided

- nan errors appears to be generated by an error of calculation in the loss function, when there wasnt enough cases of any 
  class or something alike

# -- TRAINING SCHEME FOR ANN-MLP -------------------------------------------------------------------------- #

One advantage of having callback is to set a large number of epochs and let the learning process progress indefinitely
until one condition is reach, it could be a divergence of the learning algorithm which results in a NaN reporte value
therefore trigger the TerminateOnNaN callback, other reason is the unimprovement of the accuracy metric for a continuous
number of examples, and also a high divergence in the similarity of distributions of the predicted values and the training
values. 

More on the divergence of the learning algorithm, the learning process is terminated for the entire batch whenever the cost
function presents a NaN value, possibly caused by a divergence of the learning algorithm working on the search space of the
cost function. This is implemented with TerminateOnNaN function. 

In reggards with monitoring accuracy rate dynamic under the definition of a logloss cost function, this is done with
EarlyStopping function.

ReduceLROnPlateau: Reducing the learning rate of the learning algorithm given an increasing rate of the divergence on the
distribution of the real value and the predicted value of the target variable.
