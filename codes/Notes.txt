# -- Akaike Criteria
In order to compare between models and between different parameter configurations of the same model

# -- Simpson paradox 
Simpson's paradox is one of many reasons why itâ€™s important to evaluate your models on different slices of data. Model 1 outperforms Model 2 on group A and group B separately, but model 2 can still outperform model 1 with the overall data.
https://en.wikipedia.org/wiki/Simpson%27s_paradox
Here is a multivariable procedure that explicitly maximizes classification accuracy and prevents paradoxical confounding. [5, 6]
https://www.researchgate.net/post/How_do_you_explain_higher_discrimination_using_a_SVM_score_index_unsupervised_from_a_supervised_SVM_model_using_same_dataset

# -- Usefull Difference between AUC & ACC
https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy
http://notmatthancock.github.io/2015/11/11/auc-and-accuracy.html

# -- How does batch size affect convergence of SGD and why
https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why

# -- How to Control the Stability of Training Neural Networks With the Batch Size
https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/

# -- Number of hidden layers / number of neurons & Universal Aproximation Theorem.
https://www.researchgate.net/post/How-to-decide-the-number-of-hidden-layers-and-nodes-in-a-hidden-layer
https://www.heatonresearch.com/2017/06/01/hidden-layers.html
https://en.wikipedia.org/wiki/Universal_approximation_theorem
https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

# -- RandomForest
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- T-FOLD PROCESS
# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- Define T-Fold longitudes: Quarter, Semester, Year, Bi-year, 80-20

# -- Within each T-Fold
# -- -- Separate data in 80% (in sample) - 20% (out of sample)

# -- -- (OPTION 1) From the begining Scale data with 3 options
# -- -- -- (OPTION 1) Normlize x/max(x)
# -- -- -- (OPTION 2) Standarize (x-mean(x))/sd(x)
# -- -- -- (OPTION 3) Robust Scale (x - median(x))/IQR

# -- -- -- Create Autoregressive and Hadamard features
# -- -- -- Create Symbolic Features (genetic programming)
# -- -- Continue

# -- -- (OPTION 2) After generating Symbolic Features Scale all the data with 3 options
# -- -- -- (OPTION 1) Normlize x/max(x)
# -- -- -- (OPTION 2) Standarize (x-mean(x))/sd(x)
# -- -- -- (OPTION 3) Robust Scale (x - median(x))/IQR
# -- -- Continue

# -- -- Conduct the hyperparameter optimization for all models (Genetic Algorithms with its own parameters)
# -- -- Select the fitness metric for the optimization
        (OPTION 1) AUC with train data
        (OPTION 2) AUC with test data
        (OPTION 3) mean of train and test AUC
        (OPTION 4) weighted-mean (80-20) of train and test AUC
        (OPTION 5) inv-weighted-mean (20-80) of train and test AUC

# -- -- Save the HoF with best N individuals
# -- -- Calculate model metrics for the entire Hall of Fame
# -- -- Calculate Model-inFold Performance Metric to find max and min in the fold: 
        (OPTION 1) AUC with train data
        (OPTION 2) AUC with test data
        (OPTION 3) mean of train and test AUC
        (OPTION 4) weighted-mean (80-20) of train and test AUC
        (OPTION 5) inv-weighted-mean (20-80) of train and test AUC

# -- -- Save the parameter set that produces the max, min value of the Choosen Performance Metric
# -- -- Save the number of occurrences of every parameter set (extract the modes)

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- SYMBOLIC FEATURE ENGINEERING
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Autoregressive features
# -- Hadamard features

# -- Genetic Programming for Symbolic Features
# -- -- Heuristics 
# -- -- -- Genetic Diversity
# -- -- -- Fitness for feature selection
# -- -- -- Feature parsimony

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- CALIBRATION TECHNIQUE
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Optimization Function

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- MODEL DIVERSITY
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Basic - Logistic Regression with regularization
# -- Statistical Learning - Support Vector Machines with L1 regularization
# -- Machine Learning - Aritificial Neural Network - Multi Layer Pereceptron
# -- Tree-based-ensemble method - RandomForest

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- TRADING SYSTEM DEFINITION
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Criteria 1: Data
https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5
https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-ii-volume-and-dollar-bars-6cda27e3201d

# -- Criteria 2: Signal Generation

- Reference 1
- Reference 2

# -- Criteria 3: Profit and Loss taking 

- Reference 1
- Reference 2

# -- Criteria 4: Position sizing

- Reference 1
- Reference 2

# -- (Pending) search for the max, min, mode cases among all the folds
# -- -- display metrics of de lowest min and the highest max 
# -- -- display plots

# -- (Pending) with min, max, mode cases found, evaluate with the rest of T-Folds data
# -- -- display metrics
# -- -- display plots

# -- -- DATA ANALYSIS
# -- -- -- Amount/dates of data
# -- -- -- Balance of the clases

# -- -- FEATURE ANALYSIS
# -- -- -- correlation matrix among features
# -- -- -- outliers presence in features
# -- -- -- Features parsimony (length and depth of program)

# -- -- MODEL ANALYSIS
# -- -- -- change in accuracy
# -- -- -- change in auc
# -- -- -- change in parameters

# -- PARAMETER ESTABILITY
# -- -- for every model in every T-Fold evaluate the best and worst of the HoF of the fold
# -- -- display timeseries parameters dataframe (parameters of all bests/worsts of all the folds)
# -- -- display timeseries metrics plots (of all bests/worsts of all the folds)
# -- -- -- AUC, ACC
# -- -- display all the ROC curves and average ROC curve plot for every model

# -- Possible questions to answer:
# -- -- is this method useful to find a parameter set that does not change a lot in time ?
# -- -- is this method useful to have a stable generalization error of the model ?
# -- -- does this method shows that a linear combination of non-linear features performs well
        in financial timeseries forecasting ?
# -- -- what role plays the feature complexity in this method ?
# -- -- What is the time complexity of this method ? 
# -- -- What is the memory complexity of this method ? 

# -- ANN-MLP
# -- -- HYPERPARAMETERS

# -- -- -- hidden_layer_sizes: The ith element represents the number of neurons in the ith hidden layer.
# -- -- -- alpha: L2 penalty (regularization term) parameter.
# -- -- -- activation: Activation function for the hidden layer.
# -- -- -- learning_rate_init: The initial learning rate used. It controls the step-size in updating the weights.

# -- Exploratory Data Analysis: Linear and Symbolic Features Statistics ----------------------------------- #
# -- ------------------------------------------------------------------ -------------------------------- -- #

# exploratory data analysis (base price data (nor target variable or exploratory variables))

# target variable
# -- balance (among classes)
# -- boxplot for outliers 
# -- histogram and measurement of symmetry (individual)
# -- Correlation heatmap (all)

# -- parsimony vs outliers
# -- parsimony vs symmetry

# cross analysis (Winning Fold Vs Global Test)
# -- outliers vs regularization
# -- symmetry vs regularization
# -- correlation vs regularization
# -- parsimony vs outliers
# -- parsimony vs symmetry

# model response to features from Fold to Global
# -- Regularization effects in logistic regression
# -- kernel in SVM
# -- training time in ANN-MLP


# -------- CHANGE LOG -------- #

create a new document.py file with all the codes necessary to produce the elements that will be
added to the thesis document.

