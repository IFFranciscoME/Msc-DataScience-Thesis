# -- Check for the presence of the simpson paradox 
# -- Simpson's paradox is one of many reasons why itâ€™s important to evaluate your models on different slices of data.
# -- Model 1 outperforms model 2 on group A and group B separately, but model 2 can still outperform model 1 overall.

# -- Usefull Difference between AUC & ACC
# -- ------------------------------------------------------------- ------------------------------------- -- #

https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy
http://notmatthancock.github.io/2015/11/11/auc-and-accuracy.html

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- T-FOLD PROCESS
# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- Define T-Fold longitudes: Quarter, Semester, Year, Bi-year, 80-20
# -- With each T-Fold
# -- -- Separate data in 80% (in sample) - 20% (out of sample)
# -- -- Scale data with a Standarization process (with in sample and out of sample data)
# -- -- Create Autoregressive and Hadamard features (with in sample and out of sample data)
# -- -- Create Symbolic Features (genetic programming) (with in sample and out of sample data)
# -- -- Conduct the hyperparameter optimization for all models (genetic algorithms)
# -- -- -- train with in sample and fitness with tp+tn/total
# -- -- Save the HoF with best N individuals (parameter set)
# -- -- Save the parameter set that produces the max, min value of the fitness metric
# -- -- Calculate model metrics for the entire Hall of Fame
# -- -- -- metrics: AUC and ACC

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- SYMBOLIC FEATURE ENGINEERING
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Autoregressive features
# -- Hadamard features

# -- Genetic Programming for Symbolic Features
# -- -- Heuristics 
# -- -- -- Genetic Diversity
# -- -- -- Fitness for feature selection
# -- -- -- Feature parsimony

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- CALIBRATION TECHNIQUE
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- MODEL DIVERSITY
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Basic - Logistic Regression with regularization
# -- Statistical Learning - Support Vector Machines with L1 regularization
# -- Machine Learning - Aritificial Neural Network - Multi Layer Pereceptron
# -- Tree-based-ensemble method - XGBoost trees

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- TRADING SYSTEM DEFINITION
# -- ------------------------------------------------------------- ------------------------------------- -- #



# -- (Pending) For every model, save the mode parameter set among all the T-Folds
# -- (Pending) add other t-fold sizes (bi-anual, 80-20)

# -- (Pending) search for the max, min, mode cases among all the folds
# -- -- display metrics
# -- -- display plots

# -- (Pending) with min, max, mode cases found, evaluate with the rest of T-Folds data
# -- -- display metrics
# -- -- display plots

# -- -- DATA ANALYSIS
# -- -- -- Amount/dates of data
# -- -- -- Balance of the clases

# -- -- MODEL ANALYSIS
# -- -- -- change in accuracy
# -- -- -- change in auc

# -- -- FEATURE ANALYSIS
# -- -- -- correlation matrix among features
# -- -- -- outliers presence in features
# -- -- -- Features parsimony (length and depth of program)

# -- PARAMETER ESTABILITY
# -- -- for every model in every T-Fold evaluate the best and worst of the HoF of the fold
# -- -- display timeseries parameters dataframe (parameters of all bests/worsts of all the folds)
# -- -- display timeseries metrics plots (of all bests/worsts of all the folds)
# -- -- -- AUC, ACC
# -- -- display all the ROC curves and average ROC curve plot for every model

# -- Possible questions to answer:
# -- -- is this method useful to find a parameter set that does not change a lot in time ?
# -- -- is this method useful to have a stable generalization error of the model ?
# -- -- does this method shows that a linear combination of non-linear features performs well in financial timeseries forecasting ?
# -- -- what role plays the feature complexity in this method ?
# -- -- What is the time complexity of this method ? 
# -- -- What is the memory complexity of this method ? 


# -- Exploratory Data Analysis: Linear and Symbolic Features Statistics ----------------------------------- #
# -- ------------------------------------------------------------------ -------------------------------- -- #

# exploratory visuals and metrics
# -- boxplot for outliers (individual)
# -- histogram and measurement of symmetry (individual)
# -- Correlation heatmap (all)
# -- parsimony vs outliers
# -- parsimony vs symmetry

# cross analysis (Winning Fold Vs Global Test)
# -- outliers vs regularization
# -- symmetry vs regularization
# -- correlation vs regularization
# -- parsimony vs outliers
# -- parsimony vs symmetry

# model response to features from Fold to Global
# -- Regularization effects in logistic regression
# -- kernel in SVM
# -- training time in ANN-MLP
