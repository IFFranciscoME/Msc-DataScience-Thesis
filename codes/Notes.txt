# -- Check for the presence of the simpson paradox 
# -- Simpson's paradox is one of many reasons why itâ€™s important to evaluate your models on different slices of data.
# -- Model 1 outperforms model 2 on group A and group B separately, but model 2 can still outperform model 1 overall.

# -- Usefull Difference between AUC & ACC
# -- ------------------------------------------------------------- ------------------------------------- -- #
https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy
http://notmatthancock.github.io/2015/11/11/auc-and-accuracy.html

# -- HOW DOES BATCH SIZE AFFECT CONVERGENCE OF SGD AND WHY ?
https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why

# -- How to Control the Stability of Training Neural Networks With the Batch Size
https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/

# -- Number of hidden layers / number of neurons & Universal Aproximation Theorem.
https://www.researchgate.net/post/How-to-decide-the-number-of-hidden-layers-and-nodes-in-a-hidden-layer
https://www.heatonresearch.com/2017/06/01/hidden-layers.html
https://en.wikipedia.org/wiki/Universal_approximation_theorem
https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

# -- XGBoost
https://github.com/dmlc/xgboost


# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- T-FOLD PROCESS
# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- Define T-Fold longitudes: Quarter, Semester, Year, Bi-year, 80-20
# -- With each T-Fold
# -- -- Separate data in 80% (in sample) - 20% (out of sample)
# -- -- Scale data with a Standarization process (with in sample and out of sample data)
# -- -- Create Autoregressive and Hadamard features (with in sample and out of sample data)
# -- -- Create Symbolic Features (genetic programming) (with in sample and out of sample data)
# -- -- Conduct the hyperparameter optimization for all models (genetic algorithms)
# -- -- -- train with in sample and fitness with tp+tn/total
# -- -- Save the HoF with best N individuals (parameter set)
# -- -- Save the parameter set that produces the max, min value of the fitness metric
# -- -- Calculate model metrics for the entire Hall of Fame
# -- -- -- metrics: AUC and ACC

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- SYMBOLIC FEATURE ENGINEERING
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Autoregressive features
# -- Hadamard features

# -- Genetic Programming for Symbolic Features
# -- -- Heuristics 
# -- -- -- Genetic Diversity
# -- -- -- Fitness for feature selection
# -- -- -- Feature parsimony

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- CALIBRATION TECHNIQUE
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Optimization Function

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- MODEL DIVERSITY
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Basic - Logistic Regression with regularization
# -- Statistical Learning - Support Vector Machines with L1 regularization
# -- Machine Learning - Aritificial Neural Network - Multi Layer Pereceptron
# -- Tree-based-ensemble method - XGBoost trees

# -- ------------------------------------------------------------- ------------------------------------- -- #
# -- TRADING SYSTEM DEFINITION
# -- ------------------------------------------------------------- ------------------------------------- -- #

# -- Criteria 1: Data
https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5
https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-ii-volume-and-dollar-bars-6cda27e3201d

# -- Criteria 2: Signal Generation
# -- Criteria 3: Profit and Loss taking 
# -- Criteria 4: Position sizing


# -- (Pending) For every model, save the mode parameter set among all the T-Folds
# -- (Pending) add other t-fold sizes (bi-anual, 80-20)

# -- (Pending) search for the max, min, mode cases among all the folds
# -- -- display metrics
# -- -- display plots

# -- (Pending) with min, max, mode cases found, evaluate with the rest of T-Folds data
# -- -- display metrics
# -- -- display plots

# -- -- DATA ANALYSIS
# -- -- -- Amount/dates of data
# -- -- -- Balance of the clases

# -- -- MODEL ANALYSIS
# -- -- -- change in accuracy
# -- -- -- change in auc

# -- -- FEATURE ANALYSIS
# -- -- -- correlation matrix among features
# -- -- -- outliers presence in features
# -- -- -- Features parsimony (length and depth of program)

# -- PARAMETER ESTABILITY
# -- -- for every model in every T-Fold evaluate the best and worst of the HoF of the fold
# -- -- display timeseries parameters dataframe (parameters of all bests/worsts of all the folds)
# -- -- display timeseries metrics plots (of all bests/worsts of all the folds)
# -- -- -- AUC, ACC
# -- -- display all the ROC curves and average ROC curve plot for every model

# -- Possible questions to answer:
# -- -- is this method useful to find a parameter set that does not change a lot in time ?
# -- -- is this method useful to have a stable generalization error of the model ?
# -- -- does this method shows that a linear combination of non-linear features performs well in financial timeseries forecasting ?
# -- -- what role plays the feature complexity in this method ?
# -- -- What is the time complexity of this method ? 
# -- -- What is the memory complexity of this method ? 


# -- ANN-MLP
# -- -- HYPERPARAMETERS

# -- -- -- hidden_layer_sizes: The ith element represents the number of neurons in the ith hidden layer.
# -- -- -- alpha: L2 penalty (regularization term) parameter.
# -- -- -- activation: Activation function for the hidden layer.
# -- -- -- learning_rate_init: The initial learning rate used. It controls the step-size in updating the weights.

# -- Exploratory Data Analysis: Linear and Symbolic Features Statistics ----------------------------------- #
# -- ------------------------------------------------------------------ -------------------------------- -- #

# exploratory visuals and metrics
# -- boxplot for outliers (individual)
# -- histogram and measurement of symmetry (individual)
# -- Correlation heatmap (all)
# -- parsimony vs outliers
# -- parsimony vs symmetry

# cross analysis (Winning Fold Vs Global Test)
# -- outliers vs regularization
# -- symmetry vs regularization
# -- correlation vs regularization
# -- parsimony vs outliers
# -- parsimony vs symmetry

# model response to features from Fold to Global
# -- Regularization effects in logistic regression
# -- kernel in SVM
# -- training time in ANN-MLP
