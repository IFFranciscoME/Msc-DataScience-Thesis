
# -- Akaike Criteria
In order to compare between models and between different parameter configurations of the same model

# -- Simpson paradox 
Simpson's paradox is one of many reasons why itâ€™s important to evaluate your models on different slices of data. Model 1 outperforms Model 2 on group A and group B separately, but model 2 can still outperform model 1 with the overall data.
https://en.wikipedia.org/wiki/Simpson%27s_paradox
Here is a multivariable procedure that explicitly maximizes classification accuracy and prevents paradoxical confounding. [5, 6]
https://www.researchgate.net/post/How_do_you_explain_higher_discrimination_using_a_SVM_score_index_unsupervised_from_a_supervised_SVM_model_using_same_dataset

# -- Usefull Difference between AUC & ACC
https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy
http://notmatthancock.github.io/2015/11/11/auc-and-accuracy.html

# -- How does batch size affect convergence of SGD and why
https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why

# -- How to Control the Stability of Training Neural Networks With the Batch Size
https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/

# -- Number of hidden layers / number of neurons & Universal Aproximation Theorem.
https://www.researchgate.net/post/How-to-decide-the-number-of-hidden-layers-and-nodes-in-a-hidden-layer
https://www.heatonresearch.com/2017/06/01/hidden-layers.html
https://en.wikipedia.org/wiki/Universal_approximation_theorem
https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

# -- RandomForest
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html


# -- ------------------------------------------------------------------------------------ T-FOLD PROCESS -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Define T-Fold longitudes: Quarter, Semester, Year, Bi-year, 80-20

# -- Within each T-Fold
# -- -- Separate data in 80% (in sample) - 20% (out of sample)

# -- -- (OPTION 1) From the begining Scale data with 3 options
# -- -- -- (OPTION 1) Normlize x/max(x)
# -- -- -- (OPTION 2) Standarize (x-mean(x))/sd(x)
# -- -- -- (OPTION 3) Robust Scale (x - median(x))/IQR

# -- -- -- Create Autoregressive and Hadamard features
# -- -- -- Create Symbolic Features (genetic programming)
# -- -- Continue

# -- -- (OPTION 2) After generating Symbolic Features Scale all the data with 3 options
# -- -- -- (OPTION 1) Normlize x/max(x)
# -- -- -- (OPTION 2) Standarize (x-mean(x))/sd(x)
# -- -- -- (OPTION 3) Robust Scale (x - median(x))/IQR
# -- -- Continue

# -- -- Conduct the hyperparameter optimization for all models (Genetic Algorithms with its own parameters)
# -- -- Select the fitness metric for the optimization
        (OPTION 1) AUC with train data
        (OPTION 2) AUC with test data
        (OPTION 3) mean of train and test AUC
        (OPTION 4) weighted-mean (80-20) of train and test AUC
        (OPTION 5) inv-weighted-mean (20-80) of train and test AUC

# -- -- Save the HoF with best N individuals
# -- -- Calculate model metrics for the entire Hall of Fame
# -- -- Calculate Model-inFold Performance Metric to find max and min in the fold: 
        (OPTION 1) AUC with train data
        (OPTION 2) AUC with test data
        (OPTION 3) mean of train and test AUC
        (OPTION 4) weighted-mean (80-20) of train and test AUC
        (OPTION 5) inv-weighted-mean (20-80) of train and test AUC

# -- -- Save the parameter set that produces the max, min value of the Choosen Performance Metric
# -- -- Save the number of occurrences of every parameter set (extract the modes)


# -- ---------------------------------------------------------------------- SYMBOLIC FEATURE ENGINEERING -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Autoregressive features
# -- Hadamard features

# -- Genetic Programming for Symbolic Features
# -- -- iterative genetic parameters: Functions, Population, Generations, Tournament, HoF, output features
# -- -- Heuristics 
# -- -- -- Fitness for feature selection: Largest pearson coefficient between
		   symbolic continuous variables & target binary variable
# -- -- -- Feature parsimony

# -- ----------------------------------------------------------------------------- CALIBRATION TECHNIQUE -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Optimization Function

# -- ----------------------------------------------------------------------------------- MODEL DIVERSITY -- # 
# -- --------------------------------------------------------------------------------------------------- -- #

# -- Basic - Logistic Regression with regularization
# -- Statistical Learning - Support Vector Machines with L1 regularization
# -- Machine Learning - Aritificial Neural Network - Multi Layer Pereceptron
# -- Tree-based-ensemble method - RandomForest

# -- ------------------------------------------------------------------------- TRADING SYSTEM DEFINITION -- # 
# ------------------------------------------------------------------------------------------- ---------- -- #

# -- Criteria 1: Data
https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5
https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-ii-volume-and-dollar-bars-6cda27e3201d

# -- Criteria 2: Signal Generation

- Reference 1
- Reference 2

# -- Criteria 3: Profit and Loss taking 

- Reference 1
- Reference 2

# -- Criteria 4: Position sizing

- Reference 1
- Reference 2

# ------------------------------------------------------------------------------------ SUBJECTS BY CLASS -- #
# ------------------------------------------------------------------------------------------- ---------- -- #

- Machine Learning
- - (Done) Used K-fold Cross-Validation technique as a basis and proposed some variations in order to use
	it in financial time series data, this in line with what is included in hastie & tibshirani 
	Elements of Statistical Learning.

- - (Pending) Gradient descent vs Genetic Algorithm as methods of learning for the models
    SAGA: A Fast Incremental Gradient Method WithSupport for Non-Strongly Convex CompositeObjectives.
    SGD: Stochastic Gradient Descent.
    Loss functions for binary classification models ??

- - (Extra) Regularization factors L1, and L2 combined in Elastic Net

- Time Series
- - (Done) Base properties of time series data
- - (Pending) AIC and BIC criterias for model selection

- Convex Optimization
- - (Pending) Convexity of loss function utilized for training the models
- - (Done) Logistic Regression model
- - (Done) Support Vector Machines model
- - (Done) Regularization factors L1, and L2 combined in Elastic Net

- Predictive Modeling
- - (pending) Data profiling
- - (pending) RandomForest model use for binary classification

- Analysis and Design of Algorithms
- - (Pending) Complexity analysis of T-Fold technique
- - (Pending) Complexity analysis of predictive models
- - (Pending) Calibration technique design and complexity analysis
- - (Pending) Genetic Algorithms 

- Previously
- - (Done) Multilayer Pereceptron model

# ------------------------------------------------------------------------------------------- CHANGE LOG -- #
# ------------------------------------------------------------------------------------------- ---------- -- #

- create a new document.py file with all the codes necessary to produce the elements that will be
  added to the thesis document.

- T-Fold complete features
- Parallelization capabilities of the project
- Log registration with every worker in the paralelization

- mapped all variations of factors to run the general experiment 
- - different types of data pre-processing (Normalize, Standarize, Robust Standarization)
- - data pre-processing order (pre/post features)
- - fitness metric in optimization process (train, test, simple, weighted, inv-weighted)

- added asyncronous parallelization capabilities for the project
- added logging method that support parallelization
- added pickle_rick data saving method that support parallelization
- added data_profile function for OHLC and timeseries data profiling

- Data profile for target variable (balance of classes)
- Data profile for feature variables (descriptive statistics)

# ---------------------------------------------------------------------------------------------- BACKLOG -- # 
# ---------------------------------------------------------------------------------------------- ------- -- #

# -- NECESSARY

- (pending) visualization: boxplot matrix with outliers for feature variables
- (pending) visualization: histograms for feature variables
- (pending) visualization: Correlation heatmap for feature variables
- (pending) table: Correlation table for feature variables vs target variable

- (pending) validation of data_scaling function in all the cases
- (pending) visual_profile for OHLC and timeseries visual profiling
- (pending) add RandomForest classification model with scikit learn
- (pending) migrate ann-mlp model from scikit learn to keras
- (pending) add criteria 3 and criteria 4 for trading simulation function
- (pending) visualization ROC curves of all HoF in the fold
- (pending) calibration 
- (pending) validate Data Profile for OHLC prices
- (pending) Data Profile for Features
- (pending) Statistical description of Genetic Programms

- (pending) extract data for mode cases
- (pending) metrics and other analytics for mode cases
- (pending) table for multi modal cases
- (pending) table for symbolic features
- (pending) Computational performance of Machine Learning System (Criteria 1 and 2 of trading system)

# -- GOOD TO HAVE
- (pending) plot for multi modal cases
- (pending) translate every comment to english
- (pending) translate every variable name to english
- (pending) Criteria 3: Profit and Loss taking (scalar, formula, model)
- (pending) Criteria 4: Positions dimensioning (scalar, formula, model)
- (pending) Financial performance of trading strategy (Criteria 3 and 4 of trading system)


***************************************************************************************** UNORGANIZED NOTES 
***********************************************************************************************************

# -- (Pending) search for the max, min, mode cases among all the folds
# -- -- display metrics of de lowest min and the highest max 
# -- -- display plots

# -- (Pending) with min, max, mode cases found, evaluate with the rest of T-Folds data
# -- -- display metrics
# -- -- display plots

# -- -- DATA ANALYSIS
# -- -- -- Amount/dates of data
# -- -- -- Balance of the clases

# -- -- FEATURE ANALYSIS
# -- -- -- correlation matrix among features
# -- -- -- outliers presence in features
# -- -- -- Features parsimony (length and depth of program)

# -- -- MODEL ANALYSIS
# -- -- -- change in accuracy
# -- -- -- change in auc
# -- -- -- change in parameters

# -- PARAMETER ESTABILITY
# -- -- for every model in every T-Fold evaluate the best and worst of the HoF of the fold
# -- -- display timeseries parameters dataframe (parameters of all bests/worsts of all the folds)
# -- -- display timeseries metrics plots (of all bests/worsts of all the folds)
# -- -- -- AUC, ACC
# -- -- display all the ROC curves and average ROC curve plot for every model

# -- Possible questions to answer:
# -- -- is this method useful to find a parameter set that does not change a lot in time ?
# -- -- is this method useful to have a stable generalization error of the model ?
# -- -- does this method shows that a linear combination of non-linear features performs well
        in financial timeseries forecasting ?
# -- -- what role plays the feature complexity in this method ?
# -- -- What is the time complexity of this method ? 
# -- -- What is the memory complexity of this method ? 

# -- ANN-MLP
# -- -- HYPERPARAMETERS

# -- -- -- hidden_layer_sizes: The ith element represents the number of neurons in the ith hidden layer.
# -- -- -- alpha: L2 penalty (regularization term) parameter.
# -- -- -- activation: Activation function for the hidden layer.
# -- -- -- learning_rate_init: The initial learning rate used. It controls the step-size in updating the weights.

# cross analysis (Winning Fold Vs Global Test)
# -- outliers vs regularization
# -- symmetry vs regularization
# -- correlation vs regularization
# -- parsimony vs outliers
# -- parsimony vs symmetry

# model response to features from Fold to Global
# -- Regularization effects in logistic regression
# -- kernel in SVM
# -- training time in ANN-MLP
