

# -- ----------------------------------------------------------------------------INTERESTING TO TRY-------- #
# -- --------------------------------------------------------------------------------------------------- -- #

# -- ALTERNATIVE OF IID ASUMPTION

idea: Could it be that the data among data-sets is statistically similar enough to justify the validity
of performing CV of some type and have useful results ?, for example a comparisson with the 
Kullback–Leibler divergence.

hypothesis: CV in FML is not useful because among splits, the data does present a divergence in
probability distribution, hence data-sets of different folds are not considered observations 
drawn from a idd process nor statistically similar enough to relax that assumption.

Experiment: Compare statistical momments among data sets (folds) 
		    (mean, variance, skew, kurtosis) + Kullback–Leibler divergence

# -- BIBLIOGRAPHY
[1] Advances in Financial Machine Learning, lopez de prado, wiley 2018

[2] 

