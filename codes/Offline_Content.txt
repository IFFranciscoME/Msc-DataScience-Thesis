

# -- ----------------------------------------------------------------------------INTERESTING TO TRY-------- #
# -- --------------------------------------------------------------------------------------------------- -- #

# -- ALTERNATIVE OF IID ASUMPTION

idea: Could it be that the data among data-sets is statistically similar enough to justify the validity
of performing CV of some type and have useful results ?, for example a comparisson with the 
Kullback–Leibler divergence.

hypothesis: CV in FML is not useful because among splits, the data does present a divergence in
probability distribution, hence data-sets of different folds are not considered observations 
drawn from a idd process nor statistically similar enough to relax that assumption.

Experiment: Compare statistical momments among data sets (folds) 
		    (mean, variance, skew, kurtosis) + Kullback–Leibler divergence

# -- BIBLIOGRAPHY
[1] Advances in Financial Machine Learning, lopez de prado, wiley 2018

[2] 


# Notas 

las 3 componentes principales de un metodo de regresion/clasificacion

- modelo predictivo
- funcion de costo
- algoritmo de aprendizaje o de entrenamiento (Como minimizar la funcion de costo)


Por que la funcion relu ?, tiene algo que ver con su derivada ? con el rango de salida dado el rango de entrada ? 
Citar a bengio ? 

una red neuronal es 
	una combinacion no lineal (funciones de activacion no lineales) de variables lineales o no lineales
	una combinacion lineal (funciones de activacion lineales) de variables lineales o no lineales

una neurona con una funcion sigmoide es como una regresion logistica

escalar las variables de entrada en una red neuronal sirve para que no se saturen los valores obtenidos por la funcion
de activacion en cada neurona, y es malo que se sature por que si esto pasa, la derivada no cambia y backprop determina
que no se actualiza el peso de la neurona por que ya no esta "aprendiendo". 

la capa de salida es la que determina que uso tiene la red neuronal

- regresion (la funcion de activacion en la capa de salida debe de ser lineal)
- clasificacion ()

funcion de costo para regresion

- MSE (offline, se entrena 1 vez)
- Average of all MSE (cuando es red que se entrena en linea, es decir, a cada dato nuevo se reentrena)

red neuronal, al entrenarse, no se garantiza que se encuentre la mejor solucion y que sea repetible el proceso de encontrar esa solucion encontrada

ALGORITMO DE APRENDIZAJE

la NN aprende en un sentido sequencial y de actualizacion, los pesos se van actualizando con base a un algoritmo de aprendizaje (gradiente descendente normalmente)

la funcion de costo es convexa respecto a los pesos

gradiente descentende tiene problemas con funciones de costo no convexas, y dado que la funcion de costo para regresion con respecto a los pesos es no convexa, 
es posible que GD sea "estanque" en minimos locales, o, que desde la inicializacion caiga en una zona plana de la funcion y como no hay mas cambio por que la 
derivada es cero, entonces se para el entrenamiento. 

backprop no es algoritmo de aprendizaje sino la logica de actualizacion de pesos

La desventaja de proponer muchas capas ocultas es que "exploten" o "desvanecer" los gradientes, esto por el efecto multiplicativo entre gradientes. .
(Cuando gradientes desvanecen) una actividad que hacer para verificar esto es comparar los pesos de las entradas no se mueven mucho respecto a los iniciales. Entonces, el mayor efecto
del proceso de aprendizaje recae mas en la capa de salida. y asi al caso contrario. (todo esto puede evitarse al normalizar las entradas, y posiblemente, normalizar la salida tambien)

# -- General Layout of document

- CHAPTER 1: INTRODUCTION

- CHAPTER 2: FINANCIAL MACHINE LEARNING
- - Review of literature of machine learning topics applied to finance problems, in particular, price prediction and investments
- - Argument: Define and address differences between ML and FML, particularly 3 in interest of this work
- - (1) Parsimony of the Features/Model, (2) Crossvalidaton, (3) Cost function, (4) Parametric Stability of Models, (5) Particularities of my work

- CHAPTER 3: FEATURE ENGINEERING 
- The use of data preprocessing techniques, genetic programming and heuristics for feature engineering

- - Data pre-processing
- - The process and benefits of symbolic variables as features
- - Argument: Heuristics/Explainability in synthetic-symbolic features

- CHAPTER 4: PREDICTIVE MODELS
- THe use of Statistical Learning and Machine Learning models for FML

- - No free lunch
- - Logistic, SVM, XGBoost, Neural Net (universal approximation theorem)
- - Regularization For Bias-Variance trade-off
- - Argument: Models of different Nature

- CHAPTER 5: OPTIMIZATION and BACKTESTING
- Bias-Variance tradeoff, overfitting, biases in optimizing models for FML
- On the conditions of applying CV in financial time series

- Crossvalidation
- - Different CV schemes
- - Sequential structure, the IID assumption, Leakage of information
- - Ensemble methods
- - Argument: T-fold

- Hyperparameter Optimization
- - Cost function (Convexity)
- - Optimization technique
- - Argument: Variations of cost functions

- CHAPTER 6: MATERIALS AND METHODS: PARALLEL PROCESSING FOR MULTI MODEL HYPERPARAMETER OPTIMIZATION
- Project structure to quasi-exhaustive train-test scenarios 

- Parallel CPU processing and cloud computing
- - T-Fold complexity (posterior analysis)
- - Train in parallel
- - Saving model hyperparameters
- - Deployment

- - Baseline (model benchmark)
- - Naive prediction (martingale property): 
https://en.wikipedia.org/wiki/Martingale_(probability_theory)
The conditional expected value of the next observation, given all the past observations, is equal to the most recent observation. 

# -- Why use the network architecture proposed
https://en.wikipedia.org/wiki/Universal_approximation_theorem

# -- How to choose activation function 
https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/

# -- Impact of errors in FML
the roll of "drawdown" of a trade that was made as a result of a prediction

# -- IDEA: ENCAPSULAR LO ENCONTRADO EN CADA CAPITULO DEL DOCUMENTO Y ESO ENVIARLO/EXPONERLO EN UN EVENTO

# -- Structuring MLP

- Number of layers: 1 to 3
- Number of hidden neurons
- Activation function per layer 
- Loss function
	
- Optimizer

	- it minimizes the cost (loss) function with respect to the weights.
	- When used a gradient based method, the gradient of the cost function can be calculated recursively
	- Is it possible to speed up the execution of Gradient Descent if the update is carried out through batches ?
	- Is it prone to converge to local minimus ?

- Hyperparameters of optimizer
- Capacity (model parameters)
- Dropout
- Regularization (weight, )

- General elements of a predictive model
- - Architecture of the model
- - Loss Function
- - Learning Algorithm

- General elements of Neural Network
- - FeedForward Multilayer (shallow) perceptron (ReLU, Sigmoid, Bias)
- - Binary crossentropy (with regularization)
- - Backpropagation (weight update) + Gradient Descent

# -- 

- tried all types of regularization: kernel_regularizer, bias_regularizer, activity_regularizer
- too much regularization provoque very bad model (all values are 1 class) and resulted in a code error

tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (ann-mlp/output_layer/Sigmoid:0) =

- removing the regularization layers but kept dropout resumed the process without code error but lower results. 

