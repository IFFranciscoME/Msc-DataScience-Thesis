

# -- ----------------------------------------------------------------------------INTERESTING TO TRY-------- #
# -- --------------------------------------------------------------------------------------------------- -- #

# -- ALTERNATIVE OF IID ASUMPTION

idea: Could it be that the data among data-sets is statistically similar enough to justify the validity
of performing CV of some type and have useful results ?, for example a comparisson with the 
Kullback–Leibler divergence.

hypothesis: CV in FML is not useful because among splits, the data does present a divergence in
probability distribution, hence data-sets of different folds are not considered observations 
drawn from a idd process nor statistically similar enough to relax that assumption.

Experiment: Compare statistical momments among data sets (folds) 
		    (mean, variance, skew, kurtosis) + Kullback–Leibler divergence

# -- BIBLIOGRAPHY
[1] Advances in Financial Machine Learning, lopez de prado, wiley 2018

[2] 


# Notas 

las 3 componentes principales de un metodo de regresion/clasificacion

- modelo predictivo
- funcion de costo
- algoritmo de aprendizaje o de entrenamiento (Como minimizar la funcion de costo)


Por que la funcion relu ?, tiene algo que ver con su derivada ? con el rango de salida dado el rango de entrada ? 
Citar a bengio ? 

una red neuronal es 
	una combinacion no lineal (funciones de activacion no lineales) de variables lineales o no lineales
	una combinacion lineal (funciones de activacion lineales) de variables lineales o no lineales

una neurona con una funcion sigmoide es como una regresion logistica

escalar las variables de entrada en una red neuronal sirve para que no se saturen los valores obtenidos por la funcion
de activacion en cada neurona, y es malo que se sature por que si esto pasa, la derivada no cambia y backprop determina
que no se actualiza el peso de la neurona por que ya no esta "aprendiendo". 

la capa de salida es la que determina que uso tiene la red neuronal

- regresion (la funcion de activacion en la capa de salida debe de ser lineal)
- clasificacion ()

funcion de costo para regresion

- MSE (offline, se entrena 1 vez)
- Average of all MSE (cuando es red que se entrena en linea, es decir, a cada dato nuevo se reentrena)

red neuronal, al entrenarse, no se garantiza que se encuentre la mejor solucion y que sea repetible el proceso de encontrar esa solucion encontrada

ALGORITMO DE APRENDIZAJE

la NN aprende en un sentido sequencial y de actualizacion, los pesos se van actualizando con base a un algoritmo de aprendizaje (gradiente descendente normalmente)

la funcion de costo es convexa respecto a los pesos

gradiente descentende tiene problemas con funciones de costo no convexas, y dado que la funcion de costo para regresion con respecto a los pesos es no convexa, 
es posible que GD sea "estanque" en minimos locales, o, que desde la inicializacion caiga en una zona plana de la funcion y como no hay mas cambio por que la 
derivada es cero, entonces se para el entrenamiento. 

backprop no es algoritmo de aprendizaje sino la logica de actualizacion de pesos

La desventaja de proponer muchas capas ocultas es que "exploten" o "desvanecer" los gradientes, esto por el efecto multiplicativo entre gradientes. .
(Cuando gradientes desvanecen) una actividad que hacer para verificar esto es comparar los pesos de las entradas no se mueven mucho respecto a los iniciales. Entonces, el mayor efecto
del proceso de aprendizaje recae mas en la capa de salida. y asi al caso contrario. (todo esto puede evitarse al normalizar las entradas, y posiblemente, normalizar la salida tambien)

# -- General Layout of document

- CHAPTER 1: INTRODUCTION

- CHAPTER 2: FINANCIAL MACHINE LEARNING
- - Review of literature of machine learning topics applied to finance problems, in particular, price prediction and investments
- - Argument: Define and address differences between ML and FML, particularly 3 in interest of this work
- - (1) Parsimony of the Features/Model, (2) Crossvalidaton, (3) Cost function, (4) Parametric Stability of Models, (5) Particularities of my work

- CHAPTER 3: FEATURE ENGINEERING 
- The use of data preprocessing techniques, genetic programming and heuristics for feature engineering

- - Data pre-processing
- - The process and benefits of symbolic variables as features
- - Argument: Heuristics/Explainability in synthetic-symbolic features

- CHAPTER 4: PREDICTIVE MODELS
- THe use of Statistical Learning and Machine Learning models for FML

- - No free lunch
- - Logistic, SVM, XGBoost, Neural Net (universal approximation theorem)
- - Regularization For Bias-Variance trade-off
- - Argument: Models of different Nature

- CHAPTER 5: OPTIMIZATION and BACKTESTING
- Bias-Variance tradeoff, overfitting, biases in optimizing models for FML
- On the conditions of applying CV in financial time series

- Crossvalidation
- - Different CV schemes
- - Sequential structure, the IID assumption, Leakage of information
- - Ensemble methods
- - Argument: T-fold

- Hyperparameter Optimization
- - Cost function (Convexity)
- - Optimization technique
- - Argument: Variations of cost functions

- CHAPTER 6: MATERIALS AND METHODS: PARALLEL PROCESSING FOR MULTI MODEL HYPERPARAMETER OPTIMIZATION
- Project structure to quasi-exhaustive train-test scenarios 

- Parallel CPU processing and cloud computing
- - T-Fold complexity (posterior analysis)
- - Train in parallel
- - Saving model hyperparameters
- - Deployment

- - Baseline (model benchmark)
- - Naive prediction (martingale property): 
https://en.wikipedia.org/wiki/Martingale_(probability_theory)
The conditional expected value of the next observation, given all the past observations, is equal to the most recent observation. 

# -- Why use the network architecture proposed
https://en.wikipedia.org/wiki/Universal_approximation_theorem

# -- How to choose activation function 
https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/

# -- Impact of errors in FML
the roll of "drawdown" of a trade that was made as a result of a prediction

# -- IDEA: ENCAPSULAR LO ENCONTRADO EN CADA CAPITULO DEL DOCUMENTO Y ESO ENVIARLO/EXPONERLO EN UN EVENTO

# -- --------------------------------------------- Crossvalidation for timeseries: Folds, Purge, Embargo -- #
# -- --------------------------------------------------------------------------------------------------- -- #

in AFML the author suggests a percentage criteria to embargo data from procedeing dataset whenever a 
preceding data set is present, i propose a unit number that represents the "memory" of the timeseries,
a more explicit way to state that embargo, which can be, for example, a PACF result and consequently the
number of statistically significant lags in the series. 

it is not necessary to perform embargo at the first fold since it has no preceding information, but to all
of the other folts it is. 

whenever this embargo result in a significative data dropping, metrics must be adjusted accordingly, like
accuracy, since the available data as "ground truth" is diminished in comparisson with the predicted values,
so it requires a double check of the amounts of data and subdataset divisions.

this applies to any price granularity since it is trated as a timeseries by its own, i.e, autocorrelation 
could be present in any price granularity, from seconds to monthly data. Hence, leakage of information
could be present and embargo is a candidate data pre processing technique to be implemented.

# -- ------------------------------------------------- Artificial Neural Network - Multilayer Perceptron -- #
# -- --------------------------------------------------------------------------------------------------- -- #

# ----- general components of modeling ----- # 
----------------------------------------------
MODEL ARCHITECTURE
LEARNING ALGORITHM
COST FUNCTION 
HYPERPARAMETERS (Learning, Architecture)
EXTRA CONSIDERATIONS (iterations, convergence)
----------------------------------------------

- Number of layers:
	- 1 to 3 hidden

- Number of hidden neurons:
	- per layer from 15 to 100

- Activation function per layer:
	- sigmoid
	- relu

- Cost function:
	- binary cross entropy

- Optimizer:
	- Gradient Descent with momentum / another 2 ?
	- it minimizes the cost (loss) function with respect to the weights.
	- When used a gradient based method, the gradient of the cost function can be calculated recursively
	- Is it possible to speed up the execution of Gradient Descent if the update is carried out through batches ?
	- Is it prone to converge to local mins ?

- Regularization:
	- for each layer it can be Kernel (weights), Kernel (bias), Activation activity (output of layer)
	- dropout: a percentage of the neurons will be set to 0

- Hyperparameters of optimizer
- Capacity (model parameters)
- Dropout
- Regularization (weight, )

- General elements of a predictive model
- - Architecture of the model
- - Loss Function
- - Learning Algorithm

- General elements of Neural Network
- - FeedForward Multilayer (shallow) perceptron (ReLU, Sigmoid, Bias)
- - Binary crossentropy (with regularization)
- - Backpropagation (weight update) + Gradient Descent

# -- 

# -- NOTES ON COMPUTATION:
- tried all types of regularization: kernel_regularizer, bias_regularizer, activity_regularizer
- too much regularization provoque very bad model (all values are 1 class) and resulted in a code error
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (ann-mlp/output_layer/Sigmoid:0) =
- removing the regularization layers but kept dropout resumed the process without code error but lower results.

# -- -- RETRACING A MODEL (GOOD TO HAVE ANALYSIS)
WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbb5961fc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

# -- NOTES ON DESIGN:
- for ANN-MLP in particular, it minimizes logloss (model cost function addressed by the model learning algorithm) but with genetic algorithms the fitness function is another one (global cost function).
In that reggard, the goal is to have a metric for the "best weights" to produce several best adjusted models that will be selected according to the global cost function, but also, a nested cost function
that implies having "the best model which when implemented produces the best results", i.e. the global cost functions is defined with parameters different from the model's hyperparameters, like the 
data preprocessing (data transforming functions).

# -- On the acc-diff , auc-diff, logloss-diff
One important remark is underfitting and overfitting, to address this, it was included a fitness metric that take de difference between a metric in train set and the same metric in the validation set, naturally
, in order to use this it has to be specified a valid set in the training process. This diff metric therefore will be used as fitness for the GA which will try to find the parameters that minimices this metric, 
and in consequence, to favor configurations that have the least difference between using the training set and validation set. When Training set performance metric is higher than validation it is interpreted 
as underfitting, and when performance metric is higher in validation set than in training set it is interpreted as overfitting, so the goal is to find such hyperparameters values to have in both a lower absolute 
value and a lower difference among them.
